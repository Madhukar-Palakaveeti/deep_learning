{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madhukar-Palakaveeti/deep_learning/blob/main/nninitial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.array([1,2,3,4])\n",
        "b = np.array([[1,2,3,4],[5,6,7,8]])\n",
        "c = np.array([[[1,2,3,4],[5,6,7,8]],[[2,3,4,5],[4,5,6,7]]])\n",
        "print(a.shape)\n",
        "print(b.shape)\n",
        "print(c.shape)"
      ],
      "metadata": {
        "id": "AhPorMVdrqoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a1087f-2836-40b7-9a1b-fd1413f5b9b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4,)\n",
            "(2, 4)\n",
            "(2, 2, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.zeros((3,2,4))\n",
        "print(z)\n",
        "print(z.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWihZK6-595M",
        "outputId": "3e995cd8-0129-421d-82a5-f60b9cfd2aa7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]]\n",
            "(3, 2, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.dot(b,a))\n",
        "print(np.dot(c,a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6S4oQLw9JOu",
        "outputId": "4fc94c00-8269-4065-d49f-5670d239fcef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30 70]\n",
            "[[30 70]\n",
            " [40 60]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "I = np.identity(4)\n",
        "print(I)\n",
        "print(c)\n",
        "print(I.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXqPLMO8_I97",
        "outputId": "ca3b280c-ffc7-4797-97f1-ff0a15608ca2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n",
            "[[[1 2 3 4]\n",
            "  [5 6 7 8]]\n",
            "\n",
            " [[2 3 4 5]\n",
            "  [4 5 6 7]]]\n",
            "(4, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.dot(c,I))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtKSlTCpAWHM",
        "outputId": "c34082ab-59d3-4a6f-e418-aea3adc99319"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1. 2. 3. 4.]\n",
            "  [5. 6. 7. 8.]]\n",
            "\n",
            " [[2. 3. 4. 5.]\n",
            "  [4. 5. 6. 7.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [1,2,3,2.5]\n",
        "weights = [[0.2,0.8,-0.5,1.0],[0.5,-0.91,0.26,-0.5],[-0.26,-0.27,0.17,0.87]]\n",
        "biases = [2,3,0.5]\n",
        "print(np.dot(weights,inputs)+biases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBS1moP5An3Y",
        "outputId": "34737cab-b596-487d-f9cf-f4179f140cbb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8   1.21  2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1,2,3,2.5],[2.0,5.0,-1.0,2.0],[-1.5,2.7,3.3,-0.8]])\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezZSHUwaDIDX",
        "outputId": "5dbe2962-5668-4a00-b7b8-9b116fdaf332"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.   2.   3.   2.5]\n",
            " [ 2.   5.  -1.   2. ]\n",
            " [-1.5  2.7  3.3 -0.8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n"
      ],
      "metadata": {
        "id": "-IuruskBTQ-d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases = np.zeros((1,n_neurons))\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.dot(inputs, self.weights)+ self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.maximum(0,inputs)\n",
        "\n",
        "class Activation_FakeSoftmax:\n",
        "  def forward(self,inputs):\n",
        "    exp_values = np.exp(inputs)\n",
        "    probabilities = exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
        "    self.output = probabilities\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self,inputs):\n",
        "    exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
        "    probabilities = exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
        "    self.output = probabilities\n",
        "\n",
        "class Loss:\n",
        "  def calculate(self,output,y):\n",
        "    sample_losses = self.forward(output,y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss\n",
        "\n",
        "class Loss_Categoricalcrossentropy(Loss): #Need to reimplement the whole loss function for just one-hot encoded true labels\n",
        "  def forward(self, y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "      negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    elif len(y_true.shape) == 2:\n",
        "      negative_log_likelihoods = -np.sum(y_true*np.log(y_pred_clipped)) #this is just a quick fix\n",
        "\n",
        "\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ttlSndBITVpY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhQrgHOJUJ7y",
        "outputId": "8f3d397d-a3bb-4a1c-e3c7-fe9d9d27c75f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.25.2)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nnfs\n",
        "nnfs.init()\n",
        "from nnfs.datasets import spiral_data"
      ],
      "metadata": {
        "id": "XjHxcOysqECb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = spiral_data(samples=100, classes=3)"
      ],
      "metadata": {
        "id": "7fqjCjNNqK7J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_layer = Layer_Dense(2,5)\n",
        "fake_layer.forward(X)\n",
        "# activation1 = Activation_ReLU()\n",
        "# activation1.forward(layer1.output)\n",
        "# print(activation1.output[:5])\n",
        "activation2 = Activation_FakeSoftmax()\n",
        "activation2.forward(fake_layer.output)\n",
        "print(\"Fake Softmax\")\n",
        "print(activation2.output[:5])\n",
        "\n",
        "activation3 = Activation_Softmax()\n",
        "activation3.forward(fake_layer.output)\n",
        "print(\"Softmax\")\n",
        "print(activation3.output[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcVPHZiwqV7B",
        "outputId": "5a3ace05-8769-41b3-d3a3-2a92018514d8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake Softmax\n",
            "[[0.2        0.2        0.2        0.2        0.2       ]\n",
            " [0.19999309 0.199994   0.1999831  0.2000191  0.20001073]\n",
            " [0.19996709 0.20001628 0.19997013 0.20001917 0.2000273 ]\n",
            " [0.19991961 0.20008954 0.19998294 0.1999674  0.20004053]\n",
            " [0.19989496 0.20011176 0.19997184 0.19996573 0.20005569]]\n",
            "Softmax\n",
            "[[0.2        0.2        0.2        0.2        0.2       ]\n",
            " [0.1999931  0.199994   0.1999831  0.2000191  0.2000107 ]\n",
            " [0.19996712 0.20001629 0.19997014 0.20001918 0.20002732]\n",
            " [0.1999196  0.20008956 0.19998293 0.1999674  0.20004052]\n",
            " [0.19989496 0.20011178 0.19997185 0.19996572 0.20005567]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Implementing the neural network with two layers and two activation functions\n",
        "layer1 = Layer_Dense(2,5)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "layer2 = Layer_Dense(5,3)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "layer1.forward(X)\n",
        "activation1.forward(layer1.output)\n",
        "print(f\"Activation1 : {activation1.output.shape}\")\n",
        "print(\"Activation 1 Transpose\")\n",
        "print(activation1.output.T.shape)\n",
        "layer2.forward(activation1.output)\n",
        "activation2.forward(layer2.output)\n",
        "print(f\"Activation2 : {activation2.output.shape}\")\n",
        "\n",
        "loss_function = Loss_Categoricalcrossentropy()\n",
        "loss = loss_function.forward(activation2.output,y)\n",
        "#print(f\"Loss:{loss.shape}\")\n",
        "y_dash = np.array([[0,0,1],[0,0,1],[0,0,1],[0,0,1],[0,0,1]])\n",
        "y_pred = activation2.output\n",
        "dz2 = y_pred[-1:-6:-1] - y_dash\n",
        "dw2 = np.dot(activation1.output[-1:-6:-1],dz2)\n",
        "#print(f\"dz2 : {dz2}\")\n",
        "#print(f\"dw2 : {dw2}\")\n",
        "\n",
        "#print(f\"bias : {layer1.biases}\")\n",
        "\n",
        "print(y_pred[-1:-6:-1])\n",
        "#print(y_pred[range(len(y_pred)),y][-1:-6:-1])\n",
        "#y_pred = np.argmax(activation2.output,axis=1)\n",
        "#accuracy = np.mean(y_pred == y)\n",
        "#print(np.clip(y_pred,0.0000001, -0.0000001 ))\n",
        "#print(y[-1:-6:-1])\n",
        "#print(f\"Accuracy:{accuracy*100}\")"
      ],
      "metadata": {
        "id": "C-PVZKqGq0g1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa110976-898b-41fb-e260-793b6d577cf3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation1 : (300, 5)\n",
            "Activation 1 Transpose\n",
            "(5, 300)\n",
            "Activation2 : (300, 3)\n",
            "[[0.33324292 0.33339962 0.33335742]\n",
            " [0.33324707 0.3334035  0.3333494 ]\n",
            " [0.33324376 0.3334005  0.33335584]\n",
            " [0.33324763 0.33340234 0.33334997]\n",
            " [0.33324632 0.33339745 0.33335623]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LOSS\n",
        "#categorical_crossentropy for multiclass classification\n",
        "# mean_squared_error for regression\n",
        "import math\n",
        "target_output = [1,0,0]\n",
        "y_pred = [0.7,0.2,0.1]\n",
        "loss = -(math.log(y_pred[0]))\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "FJVsQWLkrs9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de26dfd9-2665-4694-a4a5-bf9488edf3bb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[:5], y[:5])"
      ],
      "metadata": {
        "id": "cbmo0Rr1rtJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e005e7f8-b1e6-45b5-fddb-72f2a2201f2f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.        ]\n",
            " [0.00299556 0.00964661]\n",
            " [0.01288097 0.01556285]\n",
            " [0.02997479 0.0044481 ]\n",
            " [0.03931246 0.00932828]] [0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.size\n"
      ],
      "metadata": {
        "id": "cwSeaRZmMOIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ffbf32-5e12-4a6d-e860-6b5d26fae08c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[:5]"
      ],
      "metadata": {
        "id": "Xq31N4nvbLM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6980cf98-3847-459e-ffde-af58a0144112"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        ],\n",
              "       [0.00299556, 0.00964661],\n",
              "       [0.01288097, 0.01556285],\n",
              "       [0.02997479, 0.0044481 ],\n",
              "       [0.03931246, 0.00932828]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(1e-7)"
      ],
      "metadata": {
        "id": "Hs9oE4x6bM5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c71b61-19f1-421d-e6ac-161c4c50b46c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1e-07"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = X[-1:-6:-1]\n",
        "print(x_train)\n",
        "labels = [0,1,1,0,1]\n",
        "print(x_train[range(5), labels])"
      ],
      "metadata": {
        "id": "YKwbWT5IjJjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76bf7574-6e88-4616-c157-633ae036667a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.9427888   0.33339068]\n",
            " [-0.9793868  -0.14387996]\n",
            " [-0.95069844  0.23701589]\n",
            " [-0.9649202  -0.09613083]\n",
            " [-0.91110104  0.3011964 ]]\n",
            "[-0.9427888  -0.14387996  0.23701589 -0.9649202   0.3011964 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)"
      ],
      "metadata": {
        "id": "e0DIP2zsk3rH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92caa7a-2eb8-4f83-9687-0897b690d834"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "SUCprqU1m3HW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f36825-7793-41e4-d94f-89353da8d69a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title One-hot encoding the labels\n",
        "y_true = np.zeros((len(y), len(np.unique(y))))\n",
        "y_true[np.arange(y.size), y] = 1\n",
        "y_true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhiGXaMWrI6T",
        "outputId": "dca125fc-08bf-4967-a6b4-577e96eedac9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x2C27_UuvLJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOunjijvCFNixvURfJi03c",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}